{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e9e8aff-b981-4ed5-9e3f-fe6aa2f60768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Deequ is a library built on top of Apache Spark for defining \"unit tests for data\", which measure data quality in large datasets.\n",
    "###### Key packages for DQ unite testing:\n",
    "###### pypi package: pydeequ==1.4.0\n",
    "###### maven coordinate: com.amazon.deequ:deequ:2.0.8-spark-3.5\n",
    "###### https://mvnrepository.com/artifact/com.amazon.deequ/deequ\n",
    "###### https://central.sonatype.com/artifact/com.amazon.deequ/deequ/2.0.8-spark-3.5\n",
    "###### https://github.com/awslabs/deequ\n",
    "###### https://pydeequ.readthedocs.io/_/downloads/en/latest/pdf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311352d6-31bc-4da4-a031-0d17ec9c7e68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "data = [\n",
    "            (1, \"Alice\", \"Smith\", 25, \"alice.smith@email.com\", \"666-666-7777\", \"Female\"),\n",
    "            (2, \"Bob\", \"Brown\", 30, \"bob.brown@email.com\", \"888-888-8888\", \"Male\"),\n",
    "            (3, \"Carol\", \"Johnson\", 28, \"carol.johnson\", \"999-999-9999\", \"Female\"),\n",
    "            (4, \"Dave\", \"Wilson\", 0, \"dave.wilson@email.com\", \"+1234567893\", \"Male\")\n",
    "        ]\n",
    "        \n",
    "        # Schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone_number\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True)\n",
    "])\n",
    "# Crea DataFrame\n",
    "df_demo = spark.createDataFrame(data, schema) \n",
    "df_demo.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ec4baa-6a17-41af-8e80-43601de83fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set spark version variable\n",
    "\n",
    "%env SPARK_VERSION=3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b50bc3-48b2-45a0-95ab-39e3bcec2fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydeequ.analyzers import *\n",
    "import pydeequ\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark_conf = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "        .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "analysisResult = AnalysisRunner(spark_conf) \\\n",
    "                    .onData(df_demo) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Completeness(\"id\")) \\\n",
    "                    .addAnalyzer(Distinctness(\"first_name\")) \\\n",
    "                    .addAnalyzer(Uniqueness([\"first_name\"])) \\\n",
    "                    .addAnalyzer(Uniqueness([\"last_name\", \"email\"])) \\\n",
    "                    .addAnalyzer(Compliance(\"gender\", \"gender in ('Female','Male')\")) \\\n",
    "                    .addAnalyzer(Mean(\"age\")) \\\n",
    "                    .addAnalyzer(Sum(\"id\")) \\\n",
    "                    .addAnalyzer(Maximum(\"age\")) \\\n",
    "                    .run()\n",
    "                   \n",
    "                    \n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark_conf, analysisResult)\n",
    "analysisResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd696a1a-869c-4ddc-bf80-1e9928320e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "hasSize(assertion, hint=None)\n",
    "\n",
    "Creates a constraint that calculates the data frame size and runs the assertion on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faddea02-fd32-4148-ae86-dfa11d231ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.hasSize(lambda x: x == 4))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark_conf, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c9c9ac2-9b17-423f-9e6a-2c43e067aa29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "isComplete(column, hint=None)\n",
    "\n",
    "Creates a constraint that asserts on a column completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375e792b-17ee-4c1c-9d5b-dae7dc8f0dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.isComplete('id'))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72909514-4cc6-4c94-9f08-d1dd4e34930d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "areComplete(columns, hint=None)\n",
    "\n",
    "Creates a constraint that asserts completion in combined set of columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e9e249-901a-4c57-95ff-069982472ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.areComplete(['id','age']))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95a9208e-856c-425a-b73f-026a910d703a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "hasCompleteness(column, assertion, hint=None)\n",
    "\n",
    "Creates a constraint that asserts column completion. Uses the given history selection strategy to retrieve\n",
    "historical completeness values on this column from the history provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c7315d-b123-4e1d-9c3d-909d70bf131c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.hasCompleteness('id',lambda x: x >= 0.7))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda4b301-3dcd-4dbe-aa98-f9e4e3a7eecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "hasUniqueness(columns, assertion, hint=None)\n",
    "\n",
    "Creates a constraint that asserts any uniqueness in a single or combined set of key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76077103-e9f2-4c1e-8bd3-ad893c69082d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.hasUniqueness(['id','age'],lambda x : x >= 0.7))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5042c79-4dbf-42ef-acb7-b05cee47ad82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "hasMin(column, assertion, hint=None)\n",
    "\n",
    "Creates a constraint that asserts on the minimum of a column. The column is contains either a long, int or\n",
    "float datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0feabe4b-1a8f-46f8-9afe-89b4909aa487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.hasMin('age',lambda x : x >= 0))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87291769-ed97-46f5-92ca-84ae546e865f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "hasMax(column, assertion, hint=None)\n",
    "\n",
    "Creates a constraint that asserts on the maximum of the column. The column contains either a long, int or\n",
    "float datatype.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62371e19-ae80-4b82-abb8-42fd04a6d5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.hasMax('age',lambda x : x >= 20))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3498a52d-e674-47fe-8eda-7dc6dae44434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "hasMean(column, assertion, hint=None)\n",
    "\n",
    "Creates a constraint that asserts on the mean of the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b401714f-01fe-4e5d-b201-225fb27e7410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.hasMean('age',lambda x : x == 30))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e56c72ea-675c-4470-a2ab-f70f3ee6ec09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "isPositive(column, assertion=None, hint=None)\n",
    "\n",
    "Creates a constraint which asserts that a column contains no negative values and is greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7af0fcbc-9a92-4cf5-8b41-9b6a8a7a0738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.isPositive('age')\\\n",
    "                 .isPositive('id'))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80322dd1-e7b7-4096-ae2f-5790e4a1a05e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.isPositive('age',lambda x : x ==0.1))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b6a4f7-f547-4f76-ae28-0aacd8c47dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "isNonNegative(column, assertion=None, hint=None)\n",
    "\n",
    "Creates a constraint which asserts that a column contains no negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ff15d9-0f17-4381-819c-3c8b3d7fcfbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.isNonNegative('age')\\\n",
    "                 .isNonNegative('id'))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb4fa0a8-083e-4ca7-849f-cbaf925c9cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "isContainedIn(column, allowed_values, assertion=None, hint=None)\n",
    "\n",
    "Asserts that every non-null value in a column is contained in a set of predefined values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53ebf9a-b7a3-48db-85af-6d761ce0a1d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.isContainedIn(\"gender\", [\"Female\", \"Male\"]))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f72b58fe-80bb-4658-af97-f496a8b63b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "hasPattern(column, pattern, assertion=None, name=None, hint=None)\n",
    "\n",
    "Checks for pattern compliance. Given a column name and a regular expression, defines a Check on the\n",
    "average compliance of the columnâ€™s values to the regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359505cb-58d0-4c4f-908b-9fc208052c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.hasPattern(\"email\",r\"a*\",lambda x:x>=0.1))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5d9096f-d951-41a4-b82b-b43f00a1ffe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "hasDataType(column, datatype: ConstrainableDataTypes, assertion=None, hint=None)\n",
    "\n",
    "Check to run against the fraction of rows that conform to the given data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76bc7707-9589-4c19-b851-938e865b7ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "check = Check(spark_conf, CheckLevel.Warning, \"Demo\")\n",
    "checkResult = VerificationSuite(spark_conf) \\\n",
    " .onData(df_demo) \\\n",
    " .addCheck(check.hasDataType(\"id\",ConstrainableDataTypes.String))\\\n",
    " .run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c40e5f9-8cb0-4ab8-9b89-40efcc1af01e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Demo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268840a6-9884-4910-8c96-7f64de9d107b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pydeequ\n",
    "import json\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import *\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from typing import  Dict, List\n",
    "from datetime import datetime\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# os.environ['SPARK_VERSION'] = '3.2'\n",
    "spark_conf = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "        .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "class Metadata:\n",
    "\n",
    "    def __init__(self, rule_id: str, cols: str,dimension: str):\n",
    "        self.rule_id = rule_id\n",
    "        self.cols = cols\n",
    "        self.dimension = dimension\n",
    "        self.datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, indent=4)\n",
    "\n",
    "\n",
    "class PyDQChecker:\n",
    "    \n",
    "    def check_completeness(params: Dict) -> None:\n",
    "        metadata = Metadata(params[\"rule_id\"],params[\"input_attributes\"],params[\"dimension\"])\n",
    "        strategy =  params[\"strategy\"]\n",
    "        cols = params[\"input_attributes\"]\n",
    "        threshold = params.get(\"threshold\") or 1.0\n",
    "        check = Check(spark_conf, strategy, metadata.to_json())\n",
    "        check.haveCompleteness(cols, lambda x: x == threshold)\n",
    "        return check\n",
    "     \n",
    "    \n",
    "    def check_uniquiness(params: Dict) -> None:\n",
    "        metadata = Metadata(params[\"rule_id\"],params[\"input_attributes\"],params[\"dimension\"])\n",
    "        strategy =  params[\"strategy\"]\n",
    "        cols = params[\"input_attributes\"]\n",
    "        threshold = params.get(\"threshold\") or 1.0\n",
    "        check = Check(spark_conf, strategy, metadata.to_json())\n",
    "        check.hasUniqueness(cols, lambda x: x == threshold)\n",
    "        return check\n",
    "            \n",
    "    \n",
    "    def check_pattern(params: Dict) -> None:\n",
    "        metadata = Metadata(params[\"rule_id\"],params[\"input_attributes\"],params[\"dimension\"])\n",
    "        strategy =  params[\"strategy\"]\n",
    "        pattern = params[\"pattern\"]\n",
    "        cols = params[\"input_attributes\"]\n",
    "        threshold = params.get(\"threshold\") or 1.0\n",
    "        check = Check(spark_conf, strategy, metadata.to_json())\n",
    "        for col in cols:\n",
    "            check.hasPattern(col, pattern, lambda x: x >= threshold)\n",
    "        return check\n",
    "    \n",
    "    \n",
    "    def check_email(params: Dict) -> None:\n",
    "        metadata = Metadata(params[\"rule_id\"],params[\"input_attributes\"],params[\"dimension\"])\n",
    "        strategy =  params[\"strategy\"]\n",
    "        cols = params[\"input_attributes\"]\n",
    "        threshold = params.get(\"threshold\") or 1.0\n",
    "        check = Check(spark_conf, strategy, metadata.to_json())\n",
    "        for col in cols:\n",
    "            check.containsEmail(col, lambda x: x == threshold)\n",
    "        return check\n",
    "    \n",
    "    def check_isContainedIn(params: Dict) -> None:\n",
    "        metadata = Metadata(params[\"rule_id\"],params[\"input_attributes\"],params[\"dimension\"])\n",
    "        strategy =  params[\"strategy\"]\n",
    "        cols = params[\"input_attributes\"]\n",
    "        threshold = params.get(\"threshold\") or 1.0\n",
    "        contained_values = params.get(\"contained_values\") or None\n",
    "        check = Check(spark_conf, strategy, metadata.to_json())\n",
    "        for col in cols:\n",
    "            check.isContainedIn(col, contained_values)\n",
    "        return check\n",
    "    \n",
    "    def check_isPositive(params: Dict) -> None:\n",
    "        metadata = Metadata(params[\"rule_id\"],params[\"input_attributes\"],params[\"dimension\"])\n",
    "        strategy =  params[\"strategy\"]\n",
    "        cols = params[\"input_attributes\"]\n",
    "        threshold = params.get(\"threshold\") or 1.0\n",
    "        check = Check(spark_conf, strategy, metadata.to_json())\n",
    "        for col in cols:\n",
    "            check.isPositive(col,lambda x : x >= threshold)\n",
    "        return check\n",
    "    \n",
    "    def check_hasDataType(params: Dict) -> None:\n",
    "        metadata = Metadata(params[\"rule_id\"],params[\"input_attributes\"],params[\"dimension\"])\n",
    "        strategy =  params[\"strategy\"]\n",
    "        cols = params[\"input_attributes\"]\n",
    "        datatype = params.get(\"datatype\") or None\n",
    "        check = Check(spark_conf, strategy, metadata.to_json())\n",
    "        for col in cols:\n",
    "            check.hasDataType(col,datatype)\n",
    "        return check\n",
    "    \n",
    "    def run(df: DataFrame, checks: List):\n",
    "        suite = VerificationSuite(spark_conf).onData(df)\n",
    "        for check in checks:\n",
    "            suite.addCheck(check)\n",
    "        check_result = suite.run()\n",
    "        checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, check_result)\n",
    "        return checkResult_df\n",
    "\n",
    "\n",
    "class DataQualityRules:\n",
    "    def get_rules(self):\n",
    "        return [\n",
    "        {\n",
    "            \"rule_id\": \"COM.RULE.001\",\n",
    "            \"input_attributes\": [\"age\"],\n",
    "            \"strategy\": CheckLevel.Warning,\n",
    "            \"pattern\": \"\",\n",
    "            \"operation\": PyDQChecker.check_completeness,\n",
    "            \"dimension\": \"Completeness\",\n",
    "            \"datetime\": datetime.now()\n",
    "           \n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"UNQ.RULE.001\",\n",
    "            \"input_attributes\": [\"last_name\"],\n",
    "            \"strategy\": CheckLevel.Warning,\n",
    "            \"pattern\": \"\",\n",
    "            \"operation\": PyDQChecker.check_uniquiness,\n",
    "            \"dimension\": \"Uniqueness\",\n",
    "            \"datetime\": datetime.now()\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"VAL.RULE.001\",\n",
    "            \"input_attributes\": [\"first_name\", \"last_name\"],\n",
    "            \"strategy\": CheckLevel.Warning,\n",
    "            \"pattern\": \"^[a-zA-Z]+$\",\n",
    "            \"operation\": PyDQChecker.check_pattern,\n",
    "            \"dimension\": \"Validity\",\n",
    "            \"datetime\": datetime.now()\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\": \"VAL.RULE.002\",\n",
    "            \"input_attributes\": [\"phone_number\"],\n",
    "            \"strategy\": CheckLevel.Warning,\n",
    "            \"pattern\": \"^(\\+?\\d{1,3}( )?)?[- .]?((\\(\\d{3}\\))|\\d{3})[- .]?\\d{3}[- .]?\\d{4}$\",\n",
    "            \"operation\": PyDQChecker.check_pattern,\n",
    "            \"dimension\": \"Validity\",\n",
    "            \"threshold\": 0.7,\n",
    "            \"datetime\": datetime.now()\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\":  \"VAL.RULE.003\",\n",
    "            \"input_attributes\": [\"email\"],\n",
    "            \"strategy\": CheckLevel.Warning,\n",
    "            \"pattern\": \"\",\n",
    "            \"operation\": PyDQChecker.check_email,\n",
    "            \"dimension\": \"Validity\",\n",
    "            \"threshold\": 0.5,\n",
    "            \"datetime\": datetime.now()\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\":  \"VAL.RULE.004\",\n",
    "            \"input_attributes\": [\"gender\"],\n",
    "            \"strategy\": CheckLevel.Warning,\n",
    "            \"pattern\": \"\",\n",
    "            \"operation\": PyDQChecker.check_isContainedIn,\n",
    "            \"dimension\": \"Validity\",\n",
    "            \"datetime\": datetime.now(),\n",
    "            \"contained_values\": [\"Female\", \"Male\",\"Other\"],\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\":  \"POS.RULE.001\",\n",
    "            \"input_attributes\": [\"id\"],\n",
    "            \"strategy\": CheckLevel.Warning,\n",
    "            \"pattern\": \"\",\n",
    "            \"operation\": PyDQChecker.check_isPositive,\n",
    "            \"dimension\": \"Positiveness\",\n",
    "            \"datetime\": datetime.now()\n",
    "        },\n",
    "        {\n",
    "            \"rule_id\":  \"DT.RULE.001\",\n",
    "            \"input_attributes\": [\"first_name\"],\n",
    "            \"strategy\": CheckLevel.Warning,\n",
    "            \"pattern\": \"\",\n",
    "            \"operation\": PyDQChecker.check_hasDataType,\n",
    "            \"dimension\": \"DataTypeValidation\",\n",
    "            \"datetime\": datetime.now(),\n",
    "            \"datatype\": ConstrainableDataTypes.Integral\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "class DQValidator:\n",
    "\n",
    "    def __init__(self, catalog: str, schema: str, table_name: str):\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.table_name = table_name\n",
    "        self.report_suffix = \"data_quality_report\"\n",
    "        self.metadata_schema = StructType([\n",
    "            StructField(\"rule_id\", StringType(), True),\n",
    "            StructField(\"cols\", StringType(), True),\n",
    "            StructField(\"dimension\", StringType(), True),\n",
    "            StructField(\"datetime\", TimestampType(), True)\n",
    "        ])\n",
    "\n",
    "    def execute(self, df_to_validate: DataFrame = None) -> bool:\n",
    "        dq_rules = DataQualityRules()\n",
    "        df_to_validate = df_demo\n",
    "        dq_config = dq_rules.get_rules()\n",
    "        checks = []\n",
    "        for dqc in dq_config:\n",
    "            print('dqc....',dqc)\n",
    "            dq_rule_operation = dqc[\"operation\"]\n",
    "            dqc.pop(\"operation\")\n",
    "            checks.append(dq_rule_operation(dqc))\n",
    "        \n",
    "        res_df = PyDQChecker.run(df_to_validate, checks)\n",
    "        res_df.display()\n",
    "        res_df = res_df.withColumn(\"metadata\", from_json(col(\"check\"), self.metadata_schema))\n",
    "        # Expanding the JSON data into separate columns\n",
    "        res_df = res_df.select(\"*\",\"metadata.*\").drop('check')\n",
    "        res_df.display()\n",
    "        res_df.write.mode('append').saveAsTable(f'{self.table_name}_{self.report_suffix}')\n",
    "        \n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # Data\n",
    "        data = [\n",
    "            (1, \"Alice\", \"Smith\", 25, \"alice.smith@email.com\", \"666-666-7777\", \"Female\"),\n",
    "            (2, \"Bob\", \"Brown\", 30, \"bob.brown@email.com\", \"888-888-8888\", \"Male\"),\n",
    "            (3, \"Carol\", \"Johnson\", 28, \"carol.johnson\", \"999-999-9999\", \"Female\"),\n",
    "            (-5, \"Dave\", \"Wilson\", 35, \"dave.wilson@email.com\", \"+1234567893\", \"Male\")\n",
    "        ]\n",
    "        \n",
    "        # Schema\n",
    "        schema = StructType([\n",
    "            StructField(\"id\", IntegerType(), True),\n",
    "            StructField(\"first_name\", StringType(), True),\n",
    "            StructField(\"last_name\", StringType(), True),\n",
    "            StructField(\"age\", IntegerType(), True),\n",
    "            StructField(\"email\", StringType(), True),\n",
    "            StructField(\"phone_number\", StringType(), True),\n",
    "            StructField(\"gender\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "# Create DataFrame\n",
    "        df_demo = spark.createDataFrame(data, schema) \n",
    "        validation_catalog = 'hive_metastore'\n",
    "        validation_schema = 'default'\n",
    "        validation_table_name = 'data'\n",
    "        dq_validator = DQValidator(validation_catalog, validation_schema, validation_table_name)\n",
    "        dq_validator.execute(df_demo)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "pydeeque-validator-1.1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
